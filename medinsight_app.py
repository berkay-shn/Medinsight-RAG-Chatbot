import streamlit as st
from dotenv import load_dotenv
from datasets import load_dataset
# YEN襤 MIMARIYE UYUMLU IMPORTLAR
from langchain_community.vectorstores import FAISS 
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain_core.documents import Document # LangChain Document nesnesini kullanmak kritik
import os
# --- SAYFA VE TEMA AYARLARI ---
st.set_page_config(
    page_title="Medinsight RAG Bot",
    layout="wide", # Sayfan覺n t羹m geniliini kullan覺r
    initial_sidebar_state="collapsed"
)


# --- 1. ENV & API KEY SETUP ---
load_dotenv()

if "GOOGLE_API_KEY" not in os.environ:
    st.error("GOOGLE_API_KEY not found. Please check your .env file.")
    st.stop()

# --- 2. RAG PIPELINE SETUP ---
@st.cache_resource
def setup_rag_pipeline():
    # ... (st.spinner ve data = load_dataset k覺sm覺 ayn覺 kal覺r)

    # VER襤 YKLEME VE DNTRME 
    with st.spinner("Loading the knowledge base... This may take a few minutes on the first run."):
        
        data = load_dataset("Laurent1/MedQuad-MedicalQnADataset_128tokens_max", split="train")

        docs = [] 
        
        # S羹tun isimleri Mevcut veri setine g繹re d羹zeltildi: 'text' ve 'question'
        for article in data:
            # DZELTME: Ana i癟erik s羹tunu 'text' olarak deitirildi.
            page_content = article.get("text", None) 
            
            if page_content and len(page_content) > 10:
                doc = Document(
                    page_content=page_content, 
                    metadata={
                        # DZELTME: Soru s羹tunu 'question' olarak deitirildi.
                        "title": article.get("question", "Bal覺k Yok"), 
                        "source": article.get("url", "Bilinmiyor"),
                        "qtype": article.get("qtype", "Genel")
                    }
                )
                docs.append(doc)
    
    # Hata Kontrol羹: Eer hi癟 belge y羹klenmezse (繹rnein sadece 10 karakterden k覺sa cevaplar varsa)
    if not docs:
        raise ValueError("No valid documents were created from the dataset. Check your data or filtering logic.")


    # EMBEDDINGS OLUTURMA
    with st.spinner("Loading embedding model..."):
      embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # FAISS VEKTR VER襤TABANI OLUTURMA
    with st.spinner("Creating vector database..."):
        # Dorudan LangChain Document listesini kullanmak i癟in .from_documents kullan覺l覺r, 
        # bu metin ve metadata'y覺 otomatik olarak ay覺r覺r.
        vector_store = FAISS.from_documents(docs, embeddings) 

    # RAG ZINCIRI KURULUMU 
    with st.spinner("Initializing language model and RAG chain..."):
        # LLM initialization
        llm = GoogleGenerativeAI(model="gemini-2.5-flash") # gemini-pro-latest yerine gemini-2.5-flash kullan覺ld覺 (daha h覺zl覺 ve g羹ncel)
        retriever = vector_store.as_retriever()
        
        rag_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True # Kaynak belgeleri d繹nd羹rmek i癟in
        )
        return rag_chain # Zinciri d繹nd羹r

# Initialize pipeline
try:
    rag_chain = setup_rag_pipeline()
    st.success("MedInsight is ready to answer your questions!")
except Exception as e:
    # Hata durumunda uygulama durdurulur ve hata mesaj覺 g繹sterilir.
    st.error(f"An error occurred during setup: {e}")
    st.stop()

# --- ZEL CSS ST襤L襤 (KL襤N襤K GVEN TEMASI) ---
st.markdown("""
<style>
/* 1. Bal覺k ve Genel Stil */
h1 {
    text-align: center;
    color: #007bff; /* Koyu Mavi */
    border-bottom: 2px solid #007bff;
    padding-bottom: 10px;
    font-weight: 600;
}

/* 2. Ana Uygulama Arka Plan覺 (Hafif gri) */
.main {
    background-color: #f8f9fa;
}

/* 3. Sohbet Mesaj覺 Baloncuklar覺 */
[data-testid="stChatMessage"] {
    background-color: white; /* Temiz Beyaz Arka Plan */
    padding: 15px;
    border-radius: 10px;
    margin-bottom: 10px;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1); /* Hafif G繹lgelendirme */
}

/* 4. Asistan (Bot) Cevab覺 Vurgusu */
/* Bu CSS s覺n覺f覺 Streamlit'in cevab覺 癟evreleyen div'ine 繹zeldir */
[data-testid="stChatMessage"]:nth-child(even) { /* ift say覺l覺 mesajlar (genellikle asistan) */
    background-color: #e8f4ff; /* A癟覺k Mavi arka plan */
    border-left: 5px solid #007bff; /* Yan 癟izgi ile vurgu */
}

/* 5. Kullan覺c覺 Giri Alan覺 */
.stChatInputContainer {
    padding-top: 20px;
    padding-bottom: 10px;
}

</style>
""", unsafe_allow_html=True)
# -----------------------

# STREAMLIT WEB INTERFACE
st.title("MedInsight 征")
st.markdown(
    "MedInsight is a specialized **RAG (Retrieval-Augmented Generation) Chatbot** designed to answer complex medical questions." 
    "It provides reliable and comprehensive, fact-checked answers derived strictly from the structured medical knowledge base."
)

# Initialize chat history
if 'messages' not in st.session_state:
    st.session_state.messages = []

# Display previous messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Accept new user question
user_question = st.chat_input("Ask a question about the medical articles...")

if user_question:
    # Kullan覺c覺 mesaj覺n覺 kaydet ve g繹ster
    st.chat_message("user").markdown(user_question)
    st.session_state.messages.append({"role": "user", "content": user_question})

    with st.spinner("Generating response..."):
        try:
            # LangChain'in yeni mimarisinde invoke kullan覺l覺yor.
            response_dict = rag_chain.invoke({"query": user_question}) 
            response = response_dict["result"]
            
        except Exception as e:
            response = f"An error occurred while generating the response: {e}"

    with st.chat_message("assistant"):
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})